{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_channel_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0oEI3xzrvSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "!pip install sentencepiece\n",
        "!pip install bert-for-tf2\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import bert\n",
        "import os\n",
        "from bert import BertModelLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kc72GOjSihf",
        "colab_type": "text"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkCNkhsBMDEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example data, sample from the OffensEval 2020 Danish corpus \n",
        "# The parallel data in English was translated using the Google Translate API\n",
        "\n",
        "data_file = \"example_data.tsv\"\n",
        "LAN = \"DA\"  # change this according to the language you are working with\n",
        "\n",
        "data = pd.read_csv(data_file, sep=\"\\t\", header=0, names = [\"ID\", LAN, \"label\", \"EN\"])\n",
        "\n",
        "# Map labels to 0 and 1\n",
        "mapping = {\"OFF\": 1, \"NOT\": 0}\n",
        "data[\"label\"] = data[\"label\"].apply(lambda x :mapping[x])\n",
        "\n",
        "# Divide into train and test for each language\n",
        "train, test = train_test_split(data, test_size=0.33, random_state=22)\n",
        "\n",
        "train_1 = train[[\"ID\", LAN, \"label\"]]\n",
        "train_1 = train_1.rename(columns={LAN : \"tweet\"})\n",
        "train_2 = train[[\"ID\", \"EN\", \"label\"]]\n",
        "train_2 = train_2.rename(columns={\"EN\" : \"tweet\"})\n",
        "\n",
        "test_1 = test[[\"ID\", LAN, \"label\"]]\n",
        "test_1 = test_1.rename(columns={LAN:\"tweet\"})\n",
        "test_2 = test[[\"ID\", \"EN\", \"label\"]]\n",
        "test_2 = test_2.rename(columns={\"EN\" : \"tweet\"})\n",
        "\n",
        "ID_COLUMN=\"ID\"\n",
        "TEXT_COLUMN=\"tweet\"\n",
        "LABEL_COLUMN=\"label\"\n",
        "label_list=[0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awBSgctqO_0c",
        "colab_type": "text"
      },
      "source": [
        "Import the two BERT models which will constitute the channels of the multi-channel model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orM1Vg0SNq-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name1 = \"multi_cased_L-12_H-768_A-12\"\n",
        "model_dir1 = bert.fetch_google_bert_model(model_name1, \".models\")\n",
        "model_ckpt1 = os.path.join(model_dir1, \"bert_model.ckpt\")\n",
        "\n",
        "model_name2 = \"cased_L-12_H-768_A-12\"\n",
        "model_dir2 = bert.fetch_google_bert_model(model_name2, \".models\")\n",
        "model_ckpt2 = os.path.join(model_dir2, \"bert_model.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu1vuiuiPNQn",
        "colab_type": "text"
      },
      "source": [
        "Preparation of inputs to BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOUTTN0PMrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = 128\n",
        "\n",
        "# first model\n",
        "do_lower_case1 = \"uncased\" in model_name1\n",
        "bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case1, model_ckpt1)\n",
        "vocab_file1 = os.path.join(model_dir1, \"vocab.txt\")\n",
        "tokenizer1 = bert.bert_tokenization.FullTokenizer(vocab_file1, do_lower_case1)\n",
        "\n",
        "# second model\n",
        "do_lower_case2 = \"uncased\" in model_name2\n",
        "bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case2, model_ckpt2)\n",
        "vocab_file2 = os.path.join(model_dir2, \"vocab.txt\")\n",
        "tokenizer2 = bert.bert_tokenization.FullTokenizer(vocab_file2, do_lower_case2)\n",
        "\n",
        "def prepare_input(df, tokenizer, max_seq_len):\n",
        "  x, y = [], []\n",
        "  for ndx, row in df.iterrows():\n",
        "    text, label = row[TEXT_COLUMN], row[LABEL_COLUMN]\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    if len(ids) > max_seq_len:\n",
        "      ids = ids[:max_seq_len]\n",
        "    else:\n",
        "      ids = ids + [0] * (max_seq_len - len(ids))\n",
        "    x.append(ids)\n",
        "    y.append(int(label))\n",
        "  return np.array(x), np.array(y)                         \n",
        "\n",
        "# first dataset\n",
        "((train_1_x, train_1_y), \n",
        " (test_1_x, test_1_y)) = map(prepare_input, [train_1, test_1], \n",
        "                             [tokenizer1] * 2, \n",
        "                             [max_seq_len] * 2)\n",
        "\n",
        "# second dataset\n",
        "((train_2_x, train_2_y), \n",
        " (test_2_x, test_2_y)) = map(prepare_input, [train_2, test_2], \n",
        "                             [tokenizer2] * 2, \n",
        "                             [max_seq_len] * 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb5SBqO3zpKy",
        "colab_type": "text"
      },
      "source": [
        "Creation of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVaAp1IouBJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# language model 1\n",
        "bert_params_1 = bert.params_from_pretrained_ckpt(model_dir1)\n",
        "l_bert_1 = bert.\n",
        "\n",
        "\n",
        "\n",
        "ayer.from_params(bert_params_1, dtype = \"int32\", name=\"bert1\")      \n",
        "input_ids_1 = keras.layers.Input(shape=(max_seq_len,), dtype=\"int32\", name=\"input_ids1\")\n",
        "output_1 = l_bert_1(input_ids_1)\n",
        "\n",
        "print(\"bert shape\", output_1.shape)\n",
        "pooling_1 = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output_1)  # pooling layer\n",
        "dropout_1 = keras.layers.Dropout(0.1)(pooling_1)  # dropout layer\n",
        "forward_1 = keras.layers.Dense(units=768, activation=\"relu\", \n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01))(dropout_1)\n",
        "\n",
        "# language model 2\n",
        "bert_params_2 = bert.params_from_pretrained_ckpt(model_dir2)\n",
        "l_bert_2 = bert.BertModelLayer.from_params(bert_params_2, name=\"bert2\")\n",
        "input_ids_2 = keras.layers.Input(shape=(max_seq_len,), dtype=\"int32\", name=\"input_ids2\")\n",
        "output_2 = l_bert_2(input_ids_2)\n",
        "\n",
        "print(\"bert shape\", output_2.shape)\n",
        "pooling_2 = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output_2)  # pooling layer\n",
        "dropout_2 = keras.layers.Dropout(0.1)(pooling_2)  # dropout layer\n",
        "forward_2 = keras.layers.Dense(units=768, activation=\"relu\", \n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01))(dropout_2)\n",
        "\n",
        "# add hidden states together\n",
        "added = keras.layers.add([forward_1, forward_2])\n",
        "\n",
        "# classification layer\n",
        "out = keras.layers.Dense(units=2, activation=\"softmax\", \n",
        "                            kernel_regularizer=keras.regularizers.l2(0.01))(added)\n",
        "\n",
        "model = keras.Model(inputs=[input_ids_1, input_ids_2], outputs=out)\n",
        "model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "\n",
        "# bert model weights loading\n",
        "bert.load_bert_weights(l_bert_1, model_ckpt1)\n",
        "bert.load_bert_weights(l_bert_2, model_ckpt2)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=0.00002),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\")])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZtCaDgg11Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOTAL_EPOCHS = 2\n",
        "\n",
        "model.fit([train_1_x, train_2_x], train_2_y,\n",
        "          validation_split=0.,\n",
        "          batch_size=32,\n",
        "          shuffle=True,\n",
        "          epochs=TOTAL_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUL-pe3Y2C_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict([test_1_x, test_2_x]).argmax(axis=-1)\n",
        "print(classification_report(test_2_y, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}