{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_channel_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0oEI3xzrvSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "7703cc5e-d956-42dc-b84d-dc78ca002697"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade keras\n",
        "!pip install sentencepiece\n",
        "!pip install bert-for-tf2\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import bert\n",
        "import os\n",
        "from bert import BertModelLayer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.12.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kc72GOjSihf",
        "colab_type": "text"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkCNkhsBMDEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example data, sample from the OffensEval 2020 Danish corpus \n",
        "# The parallel data in English was translated using the Google Translate API\n",
        "\n",
        "data_file = \"example_data.tsv\"\n",
        "LAN = \"DA\"  # change this according to the language you are working with\n",
        "\n",
        "data = pd.read_csv(data_file, sep=\"\\t\", header=0, names = [\"ID\", LAN, \"label\", \"EN\"])\n",
        "\n",
        "# Map labels to 0 and 1\n",
        "mapping = {\"OFF\": 1, \"NOT\": 0}\n",
        "data[\"label\"] = data[\"label\"].apply(lambda x :mapping[x])\n",
        "\n",
        "# Divide into train and test for each language\n",
        "train, test = train_test_split(data, test_size=0.33, random_state=22)\n",
        "\n",
        "train_1 = train[[\"ID\", LAN, \"label\"]]\n",
        "train_1 = train_1.rename(columns={LAN : \"tweet\"})\n",
        "train_2 = train[[\"ID\", \"EN\", \"label\"]]\n",
        "train_2 = train_2.rename(columns={\"EN\" : \"tweet\"})\n",
        "\n",
        "test_1 = test[[\"ID\", LAN, \"label\"]]\n",
        "test_1 = test_1.rename(columns={LAN:\"tweet\"})\n",
        "test_2 = test[[\"ID\", \"EN\", \"label\"]]\n",
        "test_2 = test_2.rename(columns={\"EN\" : \"tweet\"})\n",
        "\n",
        "ID_COLUMN=\"ID\"\n",
        "TEXT_COLUMN=\"tweet\"\n",
        "LABEL_COLUMN=\"label\"\n",
        "label_list=[0, 1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awBSgctqO_0c",
        "colab_type": "text"
      },
      "source": [
        "Import the two BERT models which will constitute the channels of the multi-channel model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orM1Vg0SNq-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "80a6dd56-e0ae-42cd-d0c3-8a7f260c8c60"
      },
      "source": [
        "model_name1 = \"multi_cased_L-12_H-768_A-12\"\n",
        "model_dir1 = bert.fetch_google_bert_model(model_name1, \".models\")\n",
        "model_ckpt1 = os.path.join(model_dir1, \"bert_model.ckpt\")\n",
        "\n",
        "model_name2 = \"cased_L-12_H-768_A-12\"\n",
        "model_dir2 = bert.fetch_google_bert_model(model_name2, \".models\")\n",
        "model_ckpt2 = os.path.join(model_dir2, \"bert_model.ckpt\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already  fetched:  multi_cased_L-12_H-768_A-12.zip\n",
            "already unpacked at: .models/multi_cased_L-12_H-768_A-12\n",
            "Already  fetched:  cased_L-12_H-768_A-12.zip\n",
            "already unpacked at: .models/cased_L-12_H-768_A-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu1vuiuiPNQn",
        "colab_type": "text"
      },
      "source": [
        "Preparation of inputs to BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVOUTTN0PMrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = 128\n",
        "\n",
        "# first model\n",
        "do_lower_case1 = \"uncased\" in model_name1\n",
        "bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case1, model_ckpt1)\n",
        "vocab_file1 = os.path.join(model_dir1, \"vocab.txt\")\n",
        "tokenizer1 = bert.bert_tokenization.FullTokenizer(vocab_file1, do_lower_case1)\n",
        "\n",
        "# second model\n",
        "do_lower_case2 = \"uncased\" in model_name2\n",
        "bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case2, model_ckpt2)\n",
        "vocab_file2 = os.path.join(model_dir2, \"vocab.txt\")\n",
        "tokenizer2 = bert.bert_tokenization.FullTokenizer(vocab_file2, do_lower_case2)\n",
        "\n",
        "def prepare_input(df, tokenizer, max_seq_len):\n",
        "  x, y = [], []\n",
        "  for ndx, row in df.iterrows():\n",
        "    text, label = row[TEXT_COLUMN], row[LABEL_COLUMN]\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    if len(ids) > max_seq_len:\n",
        "      ids = ids[:max_seq_len]\n",
        "    else:\n",
        "      ids = ids + [0] * (max_seq_len - len(ids))\n",
        "    x.append(ids)\n",
        "    y.append(int(label))\n",
        "  return np.array(x), np.array(y)                         \n",
        "\n",
        "# first dataset\n",
        "((train_1_x, train_1_y), \n",
        " (test_1_x, test_1_y)) = map(prepare_input, [train_1, test_1], \n",
        "                             [tokenizer1] * 2, \n",
        "                             [max_seq_len] * 2)\n",
        "\n",
        "# second dataset\n",
        "((train_2_x, train_2_y), \n",
        " (test_2_x, test_2_y)) = map(prepare_input, [train_2, test_2], \n",
        "                             [tokenizer2] * 2, \n",
        "                             [max_seq_len] * 2)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb5SBqO3zpKy",
        "colab_type": "text"
      },
      "source": [
        "Creation of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVaAp1IouBJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0975a550-1aac-4aea-a25f-aa8333fc7070"
      },
      "source": [
        "# language model 1\n",
        "bert_params_1 = bert.params_from_pretrained_ckpt(model_dir1)\n",
        "l_bert_1 = bert.\n",
        "\n",
        "\n",
        "\n",
        "ayer.from_params(bert_params_1, dtype = \"int32\", name=\"bert1\")      \n",
        "input_ids_1 = keras.layers.Input(shape=(max_seq_len,), dtype=\"int32\", name=\"input_ids1\")\n",
        "output_1 = l_bert_1(input_ids_1)\n",
        "\n",
        "print(\"bert shape\", output_1.shape)\n",
        "pooling_1 = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output_1)  # pooling layer\n",
        "dropout_1 = keras.layers.Dropout(0.1)(pooling_1)  # dropout layer\n",
        "forward_1 = keras.layers.Dense(units=768, activation=\"relu\", \n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01))(dropout_1)\n",
        "\n",
        "# language model 2\n",
        "bert_params_2 = bert.params_from_pretrained_ckpt(model_dir2)\n",
        "l_bert_2 = bert.BertModelLayer.from_params(bert_params_2, name=\"bert2\")\n",
        "input_ids_2 = keras.layers.Input(shape=(max_seq_len,), dtype=\"int32\", name=\"input_ids2\")\n",
        "output_2 = l_bert_2(input_ids_2)\n",
        "\n",
        "print(\"bert shape\", output_2.shape)\n",
        "pooling_2 = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output_2)  # pooling layer\n",
        "dropout_2 = keras.layers.Dropout(0.1)(pooling_2)  # dropout layer\n",
        "forward_2 = keras.layers.Dense(units=768, activation=\"relu\", \n",
        "                              kernel_regularizer=keras.regularizers.l2(0.01))(dropout_2)\n",
        "\n",
        "# add hidden states together\n",
        "added = keras.layers.add([forward_1, forward_2])\n",
        "\n",
        "# final forward layers\n",
        "logits = keras.layers.Dense(units=128, activation=\"relu\", \n",
        "                            kernel_regularizer=keras.regularizers.l2(0.01))(added)\n",
        "out = keras.layers.Dense(units=2, activation=\"softmax\", \n",
        "                            kernel_regularizer=keras.regularizers.l2(0.01))(logits)\n",
        "\n",
        "model = keras.Model(inputs=[input_ids_1, input_ids_2], outputs=out)\n",
        "model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "\n",
        "# bert model weights loading\n",
        "bert.load_bert_weights(l_bert_1, model_ckpt1)\n",
        "bert.load_bert_weights(l_bert_2, model_ckpt2)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr=0.00002),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\")])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 128, 768)\n",
            "bert shape (None, 128, 768)\n",
            "Done loading 196 BERT weights from: .models/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f1183e74710> (prefix:bert1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Done loading 196 BERT weights from: .models/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f117ce2e5c0> (prefix:bert2). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids1 (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_ids2 (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert1 (BertModelLayer)          (None, 128, 768)     177261312   input_ids1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bert2 (BertModelLayer)          (None, 128, 768)     107718144   input_ids2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 768)          0           bert1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 768)          0           bert2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 768)          0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 768)          590592      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 768)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          98432       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            258         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 286,259,330\n",
            "Trainable params: 286,259,330\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZtCaDgg11Wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9dd687b4-62d9-45d2-bca0-3f67009c03af"
      },
      "source": [
        "TOTAL_EPOCHS = 2\n",
        "\n",
        "model.fit([train_1_x, train_2_x], train_2_y,\n",
        "          validation_split=0.,\n",
        "          batch_size=32,\n",
        "          shuffle=True,\n",
        "          epochs=TOTAL_EPOCHS)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 18.5898 - sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 18.0060 - sparse_categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f117a2f9eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUL-pe3Y2C_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "35ea50a9-acaa-42a6-eeb2-06d93d9f0dac"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict([test_1_x, test_2_x]).argmax(axis=-1)\n",
        "print(classification_report(test_2_y, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75         3\n",
            "           1       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.60         5\n",
            "   macro avg       0.30      0.50      0.37         5\n",
            "weighted avg       0.36      0.60      0.45         5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
